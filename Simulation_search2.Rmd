---
title: "Simulation"
author: "Hamed Nilforoshan"
date: "12/29/2021"
output:
  github_document:
    pandoc_args: --webtex
---


```{r setup, include=FALSE}
library(sigmoid)
library(tidymodels)
library(tidyverse)
library(furrr)
library(nloptr)
library(BBmisc)
library(latex2exp)

# Set number of workers.
plan(multicore, workers = 4)

# Set the ggplot theme.
theme_set(theme_bw(base_size = 20))
```

## Overview

We construct a stylized example to evaluate the outcomes of varying definitions of Causal Fairness 

## Setup

Prints out the results of a given admissions method
```{r trace_results}

  print_results <- function(DF_TEST, DF_TEST_COL, N_ADMIT, RESULT_NAME){
    DF_TEST<-DF_TEST[order(DF_TEST_COL,decreasing=TRUE),]

    cat("Result Type: ", RESULT_NAME,'\n')
    cat("Students Graduated: ", sum(head(DF_TEST,n=N_ADMIT)$B_p),'\n')
    cat("Diverse Admits: ", sum(head(DF_TEST,n=N_ADMIT)$D_p),'\n')
   
  }

  return_results <- function(DF_TEST, DF_TEST_COL, N_ADMIT, RESULT_NAME){
    DF_TEST<-DF_TEST[order(DF_TEST_COL,decreasing=TRUE),]
    return(c(RESULT_NAME, "Results", sum(head(DF_TEST,n=N_ADMIT)$D_p)/1.0, sum(head(DF_TEST,n=N_ADMIT)$B_p )))
  }

```


A "book of life" is a draw of each potential outcome for a population of a given
size. We'll define a function that generates a book of life for a given set of
parameters. 

```{r book_of_life}

gen_book_of_life <- function(
  pop_size,
  ...
) {
  
  # Define the structural equations.
  f_Z <- function(u_z){
    return( ifelse( 0.25 > u_z,1,0) )
  }
  
  f_W <- function(Z, u_w){
    return( ifelse(Z*0.75 + 0.25 >  u_w,1,0) )
  }
  
  f_Y <- function(Z,W, u_y){
    return( ifelse(Z*0.25 + 0.25*W + 0.5*W*Z >  u_y,1,0) )
  }


  # Generate the book of life.
  book_of_life <- tibble(
      u_z = runif(pop_size,min=0,max=1),
      u_w = runif(pop_size,min=0,max=1),
      u_y = runif(pop_size,min=0,max=1),

      Z = f_Z(u_z), # Admissions Committee Decision 
      W = f_W(Z,u_w), # Admissions Committee Decision 
      Y = f_Y(Z, W, u_y),
      Y_1 = f_Y(Z, 1, u_y),
      Y_0 = f_Y(Z, 0, u_y),


  )
}
```


## Initialize parameters and books of life
We initialize the training population size for the book of life, the fraction of the population that is a minority (black), and the number of counterfactual draws when calculating affirmative action decisions.

We then initial two books of life. One is for training our models, and the second is for testing what happens when the models are applied to a new, unseen dataset

```{r init}
set.seed(42)

POP_SIZE = 1000


df_train <-gen_book_of_life(pop_size=100000,)

write.csv(df_train,"~/Documents/df_train.csv", row.names = FALSE)


df_test <-gen_book_of_life(pop_size=POP_SIZE_TEST,
                            frac_black=FRAC_BLACK,
                            discrete=DISCRETE,
                            e_0=E_0,
                            e_r=E_R,
                            e_noise=E_NOISE,
                            m_0=M_0,
                            m_e=M_E,
                            m_noise=M_NOISE,
                            t_0=T_0,
                            t_m=T_M,
                            t_e=T_E,
                            t_e_m=T_E_M,
                            t_noise=T_NOISE,
                            i_noise=I_NOISE,
                            a_0=A_0,
                            a_t=A_T,
                            a_r=A_R,
                            a_i=A_I,
                            b_p_reject_noise=B_P_REJECT_NOISE_SD,
                            b_p_reject_noise_mean=B_P_REJECT_NOISE_MEAN,)


FRAC_ADMIT = 0.50 # POP_SIZE_TEST_ADMIT / POP_SIZE_TEST
POP_SIZE_TEST_ADMIT = FRAC_ADMIT * POP_SIZE_TEST #sum(df_test$A)

df_train_ = filter(df_train, A == 1 )
BLACK_ADMITS = sum(df_train_$R)

HUMAN_RESULTS = return_results(df_test, df_test$A, POP_SIZE_TEST_ADMIT, 'Human Decision')
HUMAN_RESULTS_NON_RANDOM = return_results(df_test, df_test$A_prob, POP_SIZE_TEST_ADMIT, 'Human Decision')

df_train%>% select(R, A) %>%
 group_by(R) %>%
 summarize(admit_rate = mean(A))

```


## Use the biased admissions committee model as the baseline 

```{r ml_naive}
ml_naive <- glm(A ~ T + R + I_noise, data = df_train, family = "binomial", control = list(maxit = 1000))

df_test$ml_naive = plogis(predict(ml_naive,df_test))

print_results(df_test, df_test$ml_naive, POP_SIZE_TEST_ADMIT, 'ML_NAIVE')

ML_NAIVE_RESULTS = return_results(df_test, df_test$ml_naive, POP_SIZE_TEST_ADMIT, 'ML Naive')

summary(ml_naive)

```


## Use the outcomes model

```{r ml_outcomes}
df_train_ = filter(df_train, A == 1 )

ml_outcomes <- glm(B_p ~ T+R, data = df_train_, family = binomial)
#ml_outcomes <- glm(B_p ~ T+R+T*R, data = df_train_)

summary(ml_outcomes)

BOOST = 0.0

df_test$ml_outcomes = predict(ml_outcomes,df_test, type = "response") + BOOST*df_test$R
df_test$ml_outcomes_prob =  predict(ml_outcomes,df_test, type = "response") 

df_test_black = tibble(df_test)
df_test_black$R=1
df_test_black$T=df_test_black$T_black
df_test_white = tibble(df_test)
df_test_white$R=0
df_test_white$T=df_test_white$T_white

df_test$ml_outcomes_black = predict(ml_outcomes,df_test_black) + BOOST*df_test_black$R
df_test$ml_outcomes_white = predict(ml_outcomes,df_test_white)  + BOOST*df_test_white$R


THRESHOLD = quantile(df_test$ml_outcomes, 1 - FRAC_ADMIT) 
df_test$ml_outcomes_decision = ifelse(df_test$ml_outcomes > THRESHOLD,1.0,0.0)
df_test$ml_outcomes_decision_black = ifelse(df_test$ml_outcomes_black > THRESHOLD,1.0,0.0)
df_test$ml_outcomes_decision_white = ifelse(df_test$ml_outcomes_white > THRESHOLD,1.0,0.0)
df_test$ml_outcomes_decision_counterfactual = ifelse(df_test$R==1,df_test$ml_outcomes_decision_white, df_test$ml_outcomes_decision_black)

sum(df_test$ml_outcomes_decision)

df_test$R = sapply(df_test$R, as.numeric)

df_check <-  df_test%>% select(R, T, ml_outcomes_decision) %>%
 group_by(R, T) %>%
 summarize(admit_rate = mean(ml_outcomes_decision))

df_check_ <-  df_test%>% select(R, T, ml_outcomes_decision_counterfactual) %>%
 group_by(R, T) %>%
 summarize(admit_rate = mean(ml_outcomes_decision_counterfactual)) %>% setNames(paste0('cf.', names(.)))

df_c <- left_join(df_check, df_check_, by = c("R" = "cf.R", "T" = "cf.T"))


ggplot(df_c) +
geom_line(aes(x = T, y = cf.admit_rate), color = "steelblue", linetype="twodash") + 
geom_line(aes(x = T, y = admit_rate), color = "darkred") + 
facet_grid(c("R")) 


ML_BOOST_2 = return_results(df_test, df_test$ml_outcomes, POP_SIZE_TEST_ADMIT, 'BLACK + 0.36')


CF_UNFAIRNESS = mean(abs(df_c$'admit_rate' - df_c$'cf.admit_rate'))


print_results(df_test, df_test$ml_outcomes, POP_SIZE_TEST_ADMIT, 'ML_Outcomes')

summary(ml_outcomes)

df_test$ml_outcomes_div = df_test$ml_outcomes + DIVERSITY_BOOST*df_test$R

THRESHOLD = quantile(df_test$ml_outcomes_div, 1-FRAC_ADMIT) 
df_test$ml_outcomes_div_decision = ifelse(df_test$ml_outcomes_div >= THRESHOLD,1.0,0.0)


ML_BOOST_2 = return_results(df_test, df_test$ml_outcomes, POP_SIZE_TEST_ADMIT, 'BLACK + 0.35')



ML_MAX_BOARDS_PASSAGE = return_results(df_test, df_test$ml_outcomes, POP_SIZE_TEST_ADMIT, 'Max Graduation')
#expression(atop("Histogram of "*hat(mu), Bootstrap~samples*','~Allianz))


ML_MAX_UTILITY = return_results(df_test, df_test$ml_outcomes_div, POP_SIZE_TEST_ADMIT,  expression("Maximum utility (when " * lambda ~ " = 0.01)"))

ML_MAX_UTILITY

write.csv(df_test,'./df_test.csv')

```


## Enforce predictive parity on the outcomes model

```{r ml_pred_par}

# df_test$index = 1:nrow(df_test)
# df_test$ml_outcomes_pred_par_decision = 0
# 
# 
# for (coeff in 0:200) {
#  
#   max_weight = -0.10
#   weight_r = max_weight * coeff/100 -0.10
#   
#   df_test$ml_outcomes_pred_par = df_test$ml_outcomes + weight_r*df_test$R
#   
#   a<-head(df_test[order(df_test$ml_outcomes_pred_par, decreasing= T),], n = POP_SIZE_TEST_ADMIT)
#   a$ml_outcomes_pred_par_decision = 1.0
#   a = a[c("index","ml_outcomes_pred_par_decision")]
#   b = merge(x = subset(df_test,select=-c(ml_outcomes_pred_par_decision)), y = a, by = "index", all.x = TRUE)
#   b[is.na(b)] <- 0
#   ##
#   df_test = b
# 
#   #THRESHOLD = quantile(df_test$ml_outcomes_pred_par, 1-FRAC_ADMIT) 
#   #POP_SIZE_TEST_ADMIT
#   #df_test$ml_outcomes_pred_par_decision = ifelse(df_test$ml_outcomes_pred_par > THRESHOLD,1.0,0.0)
#  
#   df_pred_par <-  df_test%>% select(R, B_p, ml_outcomes_pred_par_decision) %>%
#     group_by(R, ml_outcomes_pred_par_decision) %>%
#     summarize(passage_rate = mean(B_p)) %>% filter(ml_outcomes_pred_par_decision == 0)
# 
#   
#   cat( df_pred_par$passage_rate[2] / df_pred_par$passage_rate[1])
#   cat('\n')
#   cat(coeff)
#   cat('\n')
#   
#     if (df_pred_par$passage_rate[2] / df_pred_par$passage_rate[1] > 1.0){
#     return(1.0)
#   }
# 
# }
# 
# FRAC_ADMIT
# 
# ML_PREDICT_PAR= return_results(df_test, df_test$ml_outcomes_pred_par, POP_SIZE_TEST_ADMIT, 'Counterfactual Pred. Parity')
# 
# 
# 
# df_pred_par <-  df_test%>% select(R, B_p, ml_outcomes_pred_par_decision) %>%
#     group_by(R, ml_outcomes_pred_par_decision) %>%
#     summarize(passage_rate = mean(B_p)) %>% filter(ml_outcomes_pred_par_decision == 0)
# 
# df_pred_par
# 
# df_pred_par_ml_outcomes <-  df_test%>% select(R, B_p, ml_outcomes_decision) %>%
#     group_by(R, ml_outcomes_decision) %>%
#     summarize(passage_rate = mean(B_p)) %>% filter(ml_outcomes_decision == 0)
# 
# df_pred_par_ml_outcomes
# 
# 
# df_pred_par_ml_outcomes_div <-  df_test%>% select(R, B_p, ml_outcomes_div_decision) %>%
#     group_by(R, ml_outcomes_div_decision) %>%
#     summarize(passage_rate = mean(B_p)) %>% filter(ml_outcomes_div_decision == 0)
# 
# 
# df_pred_par_ml_outcomes_div
# 
# sum(df_test$ml_outcomes_pred_par_decision * df_test$ml_outcomes_div)
# 
# sum(df_test$ml_outcomes_pred_par_decision * df_test$B_p)
# mean(df_test$ml_outcomes_pred_par_decision )
# 
# 
# decision_table <-  df_test %>% select(R, T, ml_outcomes_pred_par_decision) %>%
#     group_by(R, T) %>%
#     summarize(admit_rate = mean(ml_outcomes_pred_par_decision))
# 
# 
# 
# sum(df_test$ml_outcomes_pred_par_decision * df_test$ml_outcomes_div)
# 
# 
# sum(df_test$ml_outcomes_pred_par_decision)
# 
# ML_PREDICT_PAR
# 
# rejected <-  df_test%>%  filter(ml_outcomes_div_decision == 0)
# 
# mean(rejected$B_p)
```



## Look at probabillity distributions by race

```{r ml_outcomes_diverse}

df_test$B_p_util = df_test$B_p_raw + df_test$R * DIVERSITY_BOOST

df_test$ml_outcomes_round = round(df_test$ml_outcomes,1) + df_test$R * DIVERSITY_BOOST

df_test_view <- df_test %>%
  group_by(ml_outcomes_round, R) %>%
  summarise_at(vars(B_p_util), funs(mean(., na.rm=TRUE)))

df_test_view$color = df_test_view

ggplot(df_test_view) +
geom_line(aes(x = ml_outcomes_round, y = B_p_util, color = factor(as.factor(R))), linetype="twodash") + 
facet_grid(c("R")) 


ggplot() + 
  geom_density(data=df_test, aes(x=B_p_raw, group=factor(R), color=factor(R)), adjust=2) + 
  xlab("Contribution to Utility") +
  ylab("Density")+
  theme_classic()

```
## Run linear programs in .ipynb and get output

```{r run_commands}
system('export PATH="/Users/hamedn/opt/anaconda3/bin:$PATH"; 
       jupyter nbconvert --execute --to notebook --inplace 01-Counterfactual-Fairness.ipynb;
       jupyter nbconvert --execute --to notebook --inplace 02-Path-Specific-Fairness.ipynb;
       jupyter nbconvert --execute --to notebook --inplace 03-Equalized_Odds.ipynb;
       jupyter nbconvert --execute --to notebook --inplace 04-Principal-Fairness.ipynb;
       jupyter nbconvert --execute --to notebook --inplace 05-Counterfactual-Predictive-Parity.ipynb;')

```


```{r load_lp_results}
lp_results <- read.csv("lp_results.csv", sep = "\t")

PC_FAIR = c('Counterfactual Fairness\n& Path-Specific Fairness\n(Random)', 0, lp_results[lp_results$Policy.Name=='Counterfactual Fairness',]$Frac.Diverse.Candidates * POP_SIZE_TEST_ADMIT, lp_results[lp_results$Policy.Name=='Counterfactual Fairness',]$Total.Graduated)
EO_FAIR = c('Counterfactual Equalized Odds', 0, lp_results[lp_results$Policy.Name=='Counterfactual Equalized Odds',]$Frac.Diverse.Candidates *POP_SIZE_TEST_ADMIT , lp_results[lp_results$Policy.Name=='Counterfactual Equalized Odds',]$Total.Graduated)
PRINCIPAL_FAIR = c('Principal Fairness', 0, lp_results[lp_results$Policy.Name=='Principal Fairness',]$Frac.Diverse.Candidates *POP_SIZE_TEST_ADMIT , lp_results[lp_results$Policy.Name=='Principal Fairness',]$Total.Graduated)
ML_PREDICT_PAR = c('Counterfactual Pred. Parity', 0, lp_results[lp_results$Policy.Name=='Counterfactual Pred. Parity',]$Frac.Diverse.Candidates *POP_SIZE_TEST_ADMIT, lp_results[lp_results$Policy.Name=='Counterfactual Pred. Parity',]$Total.Graduated)

```

## Draw the pareto curve

```{r pareto_curve_02}
N=round(POP_SIZE_TEST_ADMIT)  * FRAC_BLACK * 2.0

MAX_BLACK_ADMIT = POP_SIZE_TEST_ADMIT * FRAC_BLACK* 2.0
pareto_ideal = NULL

for (num_black_admit in seq(from=1,to=as.integer(N+10),by=as.integer(N/200))){
  cat(num_black_admit)
  frac_black = num_black_admit
  
  #df_test_black<- df_test[order(df_test$L_p,decreasing=TRUE),] %>% filter( R == 1 )
  #df_test_white<- df_test[order(df_test$L_p,decreasing=TRUE),] %>% filter( R == 0 )
  #lives_saved = sum(head(df_test_black,n=num_black_admit)$B_p) + sum(head(df_test_white,n=POP_SIZE_TEST_ADMIT-num_black_admit)$B_p) 
  
  df_test_black<- df_test[order(df_test$ml_outcomes,decreasing=TRUE),] %>% filter( R == 1 )
  df_test_white<- df_test[order(df_test$ml_outcomes,decreasing=TRUE),] %>% filter( R == 0 )
  lives_saved_outcomes = (sum(head(df_test_black,n=num_black_admit)$B_p) + sum(head(df_test_white,n=POP_SIZE_TEST_ADMIT-num_black_admit)$B_p))
  
  pareto_ideal = rbind(pareto_ideal, data.frame(frac_black ,lives_saved_outcomes))
  
  #frac_black = as.double(ML_MAX_BOARDS_PASSAGE[[3]])
  #lives_saved_outcomes = as.double(ML_MAX_BOARDS_PASSAGE[[4]]) 
  #pareto_ideal = rbind(pareto_ideal, data.frame(frac_black,lives_saved_outcomes))
}




# ggplot colors
peach <- "#F8766D"
purple <- "#C77CFF"
blue <- "#00BFC4"
green <- "#00BE6C"
browngreen <- "#A3A500"
darkgreen <- "#DEAA88"
navy <- "#055C9D"
darkpurple <- "#431C53"

transparency <- 0.4
labels = c("1", "2", "3", "4", "5")



random_x <- 0.1
random_y <- 280000 

pareto_ideal$cutoff = pareto_ideal$frac_black >= as.double(ML_MAX_BOARDS_PASSAGE[[3]])
lo = loess(lives_saved_outcomes ~ frac_black, pareto_ideal,span = 0.75 )
pareto_ideal$smooth_y = predict(lo, pareto_ideal$frac_black, se = FALSE)

p <- ggplot() +
  geom_vline(xintercept = FRAC_BLACK*POP_SIZE_TEST_ADMIT, linetype = "dashed", size=1.0, colour = rgb(0, 0, 0, 255, maxColorValue = 255), alpha=0.5) +
  
  geom_line(data= filter(pareto_ideal, cutoff),aes(x=frac_black, y=smooth_y, color='ML Outcomes (Given T, R)'),size=1) + 
  geom_line(data= filter(pareto_ideal, !cutoff),aes(x=frac_black, y=smooth_y, color='ML Outcomes (Given T, R)'), linetype = "dashed",size=1) + 
  
  scale_x_continuous("Admitted Applicants from Target Group", labels = scales::comma) + 
  scale_y_continuous("Bachelor's Degree Attainment", labels = scales::comma) +
  coord_cartesian(xlim = c(0.0, 30000),ylim = c(22000, 31500),expand = FALSE) + 
  
  geom_point(aes(x = as.double(ML_MAX_BOARDS_PASSAGE[[3]]), y = as.double(ML_MAX_BOARDS_PASSAGE[[4]])), color = peach, size = 3) +
  annotate("text",size=5, x = as.double(ML_MAX_BOARDS_PASSAGE[[3]])-0.065-0.035-0.02+0.125+0.125, y = as.double(ML_MAX_BOARDS_PASSAGE[[4]])+ (-200+250+250)/ 1.0, label = ML_MAX_BOARDS_PASSAGE[[1]]) +

  geom_point(aes(x = as.double(ML_MAX_UTILITY[[3]]), y = as.double(ML_MAX_UTILITY[[4]])), color = blue, size = 3) +
  
  annotate("text",size=5, x = as.double(ML_MAX_UTILITY[[3]])-0.06+0.075-0.01, y = as.double(ML_MAX_UTILITY[[4]])-500/ 1.0, label = expression("Max Utility (for " * lambda ~  " = " * frac(1, 4) *")")) +

  #annotate("text",size=5, x = as.double(ML_MAX_UTILITY[[3]])-0.06+0.075-0.01, y = as.double(ML_MAX_UTILITY[[4]])-500/ POP_SIZE_TEST_ADMIT, label = expression(atop("Max Utility","(for " * lambda ~  " = " * frac(1, 4) *")"))) +
  

  geom_point(aes(x = as.double(PC_FAIR[[3]]), y = as.double(PC_FAIR[[4]])), color = purple, size = 3) +
  annotate("text",size=5,lineheight = 0.85,hjust = 1, x = as.double(PC_FAIR[[3]])-0.02, y = as.double(PC_FAIR[[4]]) + (-50+5000 + 100)/ POP_SIZE_TEST_ADMIT, label = PC_FAIR[[1]]) +
    
  geom_point(aes(x = as.double(PRINCIPAL_FAIR[[3]]), y = as.double(PRINCIPAL_FAIR[[4]])), color = darkgreen, size = 3) +
  annotate("text",size=5, x = as.double(PRINCIPAL_FAIR[[3]])-0.11-0.015+0.04, y = as.double(PRINCIPAL_FAIR[[4]]) +( - 50 + 100 - 500-100)/ POP_SIZE_TEST_ADMIT, label = PRINCIPAL_FAIR[[1]]) +

  geom_point(aes(x = as.double(ML_PREDICT_PAR[[3]]), y = as.double(ML_PREDICT_PAR[[4]])), color = green, size = 3) +
  annotate("text",size=5, x = as.double(ML_PREDICT_PAR[[3]])+0.13+0.02, y = as.double(ML_PREDICT_PAR[[4]]) + 50/ POP_SIZE_TEST_ADMIT, label = ML_PREDICT_PAR[[1]]) +
  
  geom_point(aes(x = as.double(EO_FAIR[[3]]), y = as.double(EO_FAIR[[4]])), color = browngreen, size = 3) +
  annotate("text",size=5,lineheight = 0.85,hjust = 1, x = as.double(EO_FAIR[[3]])-0.01, y = as.double(EO_FAIR[[4]]) +(- 750+200+80)/ POP_SIZE_TEST_ADMIT, label = EO_FAIR[[1]]) +

  scale_color_manual(
      values = c('ML Outcomes (Given T, R)'="violet" )) + 
  theme(legend.title = element_blank(),legend.position="none", axis.text=element_text(size=15), axis.title=element_text(size=18), panel.grid = element_line(color = rgb(235, 235, 235, 100, maxColorValue = 255)))
ggsave(plot=p, filename='./frontier.pdf', height=6, width=7)
p


```

